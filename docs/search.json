[
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "|2024-03-10|Solution Write-Up|\nThis competition requires participants to prepare the dataset from satellite images of San Juan pre- and post-storm. With this dataset, participants are required to classify each building in the validation dataset into one of four categories: undamaged residential buildings, damaged residential buildings, undamaged commercial buildings, and damaged commercial buildings.\n\n\n\n       \n\n\n\n\n\n\n\n\n\n\n\nClick to make it larger.\n\n\n\n|2023-12-30|Notebook|\nThe goal of this project is to detect defects in concrete structures with deep learning. The model used here is an encoder-decoder model, U-Net, developed based on EfficientNet-B1 as the backbone and trained for 60 epochs.\n\n\n\n      \n\n\n\n\n\n\n\n\n\n\n\nClick to make it larger.\n\n\n\n|2023-12-05|Competition Link|Notebook Submission|\nMy participation in a Kaggle competition Playground Series - Season 3, Episode 25, Mohs Hardness Prediction, which involves a regression problem.This competition provided me with an opportunity to learn from other experts and refine my skills. My strategy include the utilization of various tree-based models such as Gradient Boost, LightGBM, XGBoost, HistGradientBoost, and CatBoost. To enhance predictive performance, I developed a custom ensemble technique that leveraged a linear model (LADRegression) for assigning weights to the predictions generated by different models. Additionally, I incorporated an Artificial Neural Network (ANN) as a meta-learner. This component was designed to adeptly combine predictions from diverse machine learning models, contributing to the development of a robust and reliable predictive model. To validate the model’s generalization capabilities, I conducted cross-validation, ensuring its effectiveness across different datasets. Furthermore, I optimized model hyperparameters efficiently using Optuna, a tool tailored for hyperparameter tuning.\n\n\n\n      \n\n\n\n\n\n\n\n\n\n\nClick to make it larger.\n\n\n|2023-11-19|Competition Link|Notebook Submission|\nThis project is a machine learning project that we conducted for the Kaggle Playground Competition, a category of competition on Kaggle that offers short machine learning challenges. The objective of this competition was to predict the smoking status of individuals based on their bio-signals, such as systolic, cholesterol level, blood sugar, and etc. We formed a team of three members, including myself as the team leader, and we achieved a rank of 28% among 1908 teams globally. The competition provided a pre-processed dataset, so we focused on developing models with high ROC-AUC scores, which measure the ability to discriminate between smokers and non-smokers. We experimented with various algorithms, such as logistic regression, random forest, and tree-based boosting methods, including gradient boost, histogram-based gradient boost, LightGBM, XGBoost, and CatBoost. To improve AUC-ROC score and reduce variance, we created a weighted-voting ensemble that combined the three best-performing algorithms that we selected after rigorous evaluation. Our ensemble model outperformed each individual model in predicting smoking status. We fine-tuned our models using Optuna, a framework for hyperparameter optimization, to find the optimal values for each parameter. This tuning process resulted in a final submission with a ROC-AUC score of 0.87178. Although this score was slightly lower than the top submission’s 0.87946, our approach demonstrated strong predictive power and emphasized the importance of algorithm selection and ensemble techniques for reducing model variance."
  },
  {
    "objectID": "projects/index.html#machine-learning-classification-of-building-damage-in-post-storm-event-assessment-annual-ey-open-science-data-challenge-competition-2024",
    "href": "projects/index.html#machine-learning-classification-of-building-damage-in-post-storm-event-assessment-annual-ey-open-science-data-challenge-competition-2024",
    "title": "Projects",
    "section": "",
    "text": "|2024-03-10|Solution Write-Up|\nThis competition requires participants to prepare the dataset from satellite images of San Juan pre- and post-storm. With this dataset, participants are required to classify each building in the validation dataset into one of four categories: undamaged residential buildings, damaged residential buildings, undamaged commercial buildings, and damaged commercial buildings."
  },
  {
    "objectID": "projects/index.html#computer-vision-project-concrete-defects-segmentation",
    "href": "projects/index.html#computer-vision-project-concrete-defects-segmentation",
    "title": "Projects",
    "section": "",
    "text": "Click to make it larger.\n\n\n\n|2023-12-30|Notebook|\nThe goal of this project is to detect defects in concrete structures with deep learning. The model used here is an encoder-decoder model, U-Net, developed based on EfficientNet-B1 as the backbone and trained for 60 epochs."
  },
  {
    "objectID": "projects/index.html#prediction-of-mohs-hardness-with-machine-learning",
    "href": "projects/index.html#prediction-of-mohs-hardness-with-machine-learning",
    "title": "Projects",
    "section": "",
    "text": "Click to make it larger.\n\n\n\n|2023-12-05|Competition Link|Notebook Submission|\nMy participation in a Kaggle competition Playground Series - Season 3, Episode 25, Mohs Hardness Prediction, which involves a regression problem.This competition provided me with an opportunity to learn from other experts and refine my skills. My strategy include the utilization of various tree-based models such as Gradient Boost, LightGBM, XGBoost, HistGradientBoost, and CatBoost. To enhance predictive performance, I developed a custom ensemble technique that leveraged a linear model (LADRegression) for assigning weights to the predictions generated by different models. Additionally, I incorporated an Artificial Neural Network (ANN) as a meta-learner. This component was designed to adeptly combine predictions from diverse machine learning models, contributing to the development of a robust and reliable predictive model. To validate the model’s generalization capabilities, I conducted cross-validation, ensuring its effectiveness across different datasets. Furthermore, I optimized model hyperparameters efficiently using Optuna, a tool tailored for hyperparameter tuning."
  },
  {
    "objectID": "projects/index.html#binary-prediction-of-smoker-status-using-bio-signals",
    "href": "projects/index.html#binary-prediction-of-smoker-status-using-bio-signals",
    "title": "Projects",
    "section": "",
    "text": "Click to make it larger.\n\n\n|2023-11-19|Competition Link|Notebook Submission|\nThis project is a machine learning project that we conducted for the Kaggle Playground Competition, a category of competition on Kaggle that offers short machine learning challenges. The objective of this competition was to predict the smoking status of individuals based on their bio-signals, such as systolic, cholesterol level, blood sugar, and etc. We formed a team of three members, including myself as the team leader, and we achieved a rank of 28% among 1908 teams globally. The competition provided a pre-processed dataset, so we focused on developing models with high ROC-AUC scores, which measure the ability to discriminate between smokers and non-smokers. We experimented with various algorithms, such as logistic regression, random forest, and tree-based boosting methods, including gradient boost, histogram-based gradient boost, LightGBM, XGBoost, and CatBoost. To improve AUC-ROC score and reduce variance, we created a weighted-voting ensemble that combined the three best-performing algorithms that we selected after rigorous evaluation. Our ensemble model outperformed each individual model in predicting smoking status. We fine-tuned our models using Optuna, a framework for hyperparameter optimization, to find the optimal values for each parameter. This tuning process resulted in a final submission with a ROC-AUC score of 0.87178. Although this score was slightly lower than the top submission’s 0.87946, our approach demonstrated strong predictive power and emphasized the importance of algorithm selection and ensemble techniques for reducing model variance."
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum vitae",
    "section": "",
    "text": "Download Current Resume"
  },
  {
    "objectID": "blog/240218_gan-first-project/index.html",
    "href": "blog/240218_gan-first-project/index.html",
    "title": "AI Learns to Generate Synthetic Deep Space Images",
    "section": "",
    "text": "In this post, we will explore how I utilize DCGAN to generate synthetic images of deep space objects. Employing PyTorch, we implement a straightforward DCGAN model. Our objective is to train a neural network capable of producing deep space objects images. This serves as a post in practical work on using DCGAN and continuation of previous post where we learn the mathematics behind the beauty of GAN."
  },
  {
    "objectID": "blog/240218_gan-first-project/index.html#initialize-the-weights",
    "href": "blog/240218_gan-first-project/index.html#initialize-the-weights",
    "title": "AI Learns to Generate Synthetic Deep Space Images",
    "section": "Initialize the Weights",
    "text": "Initialize the Weights\nLet us define the custom weights for our layers. We define a function weigths_init to initialize the weights of our neural network layers. This function is applied to both the generator (netG) and the discriminator (netD).\n# custom weights initialization called on netG and netD\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)"
  },
  {
    "objectID": "blog/240218_gan-first-project/index.html#build-the-generator",
    "href": "blog/240218_gan-first-project/index.html#build-the-generator",
    "title": "AI Learns to Generate Synthetic Deep Space Images",
    "section": "Build the Generator",
    "text": "Build the Generator\nLet’s construct the generator model for our GAN. We begin by defining the Generator class, which comprises functions that return the following neural network architecture:\nclass Generator(nn.Module):\n    def __init__(self, ngpu):\n        super(Generator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # input is Z, going into a convolution\n            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n            nn.ReLU(True),\n            # state size. (ngf*8) x 4 x 4\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            # state size. (ngf*4) x 8 x 8\n            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            # state size. (ngf*2) x 16 x 16\n            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            # state size. (ngf) x 32 x 32\n            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # state size. (nc) x 64 x 64\n        )\n\n    def forward(self, input):\n        return self.main(input)\nNext, we instantiate the Generator class and apply the custom weight initialization defined in the weights_init function to the layers of our generator:\n# Create the generator\nnetG = Generator(ngpu).to(device)\n\n# Handle multi-gpu if desired\nif (device.type == 'cuda') and (ngpu &gt; 1):\n    netG = nn.DataParallel(netG, list(range(ngpu)))\n\n# Apply the weights_init function to randomly initialize all weights to mean=0, stdev=0.02.\nnetG.apply(weights_init)\n\n# Print the model\nprint(netG)\nLet us visualize the architecture of our generator model:\nGenerator(\n  (main): Sequential(\n    (0): ConvTranspose2d(5, 1024, kernel_size=(4, 4), stride=(1, 1), bias=False)\n    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU(inplace=True)\n    (6): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (8): ReLU(inplace=True)\n    (9): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (11): ReLU(inplace=True)\n    (12): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (13): Tanh()\n  )\n)"
  },
  {
    "objectID": "blog/240218_gan-first-project/index.html#build-the-discriminator",
    "href": "blog/240218_gan-first-project/index.html#build-the-discriminator",
    "title": "AI Learns to Generate Synthetic Deep Space Images",
    "section": "Build the Discriminator",
    "text": "Build the Discriminator\nLet’s build a corresponding discriminator class for our GAN. The discriminator is essentially a simple binary classifier that decides whether a given image is real or fake. If we examine the final output layer, we will see one neuron with a sigmoid activation function.\nclass Discriminator(nn.Module):\n    def __init__(self, ngpu):\n        super(Discriminator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # input is (nc) x 64 x 64\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x 32 x 32\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*2) x 16 x 16\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*4) x 8 x 8\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # state size. (ndf*8) x 4 x 4\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.main(input)\nNext, we instantiate the Discriminator class and apply the custom weight initialization defined in the weights_init function to the layers of our discriminator:\n# Create the Discriminator\nnetD = Discriminator(ngpu).to(device)\n\n# Handle multi-gpu if desired\nif (device.type == 'cuda') and (ngpu &gt; 1):\n    netD = nn.DataParallel(netD, list(range(ngpu)))\n\n# Apply the weights_init function to randomly initialize all weights\n#  to mean=0, stdev=0.2.\nnetD.apply(weights_init)\n\n# Print the model\nprint(netD)\nLet us visualize the architecture of our discriminator model:\nDiscriminator(\n  (main): Sequential(\n    (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n    (2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n    (5): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n    (8): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (9): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n    (11): Conv2d(1024, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n    (12): Sigmoid()\n  )\n)"
  },
  {
    "objectID": "blog/231118_helloworld/index.html",
    "href": "blog/231118_helloworld/index.html",
    "title": "Hello World!",
    "section": "",
    "text": "Over the past few months, my homepage served as a placeholder for my porfolio. As I have interest in learning and sharing knowledge, I have chosen to utilize this platform to document my data science journey. And now, here we are.\nI want to express my gratitude to Murthadza Aznam, also known as @Thaza_kun on Twitter, for introducing me to the amazing Quarto project. Be sure to explore his homepage as well! Special thanks to the Quarto project for enabling this journey. Setting everything up took just a matter of hours.\nAs I aspire to become a data scientist and AI engineer, my plan is to consistently publish valuable, insightful, and thought-provoking blog posts. Cheers!"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "Get to Know Me!",
    "section": "",
    "text": "Get to Know Me!\n    \n\n\n        \n            About\n            Welcome to my profile! I am Hazman Naim Bin Ahsan, a graduate of the University of Malaya with a Bachelor of Science in Physics. My interests are diverse, ranging from the natural sciences, including physics and mathematics, to technology, such as AI and Machine Learning. I am particularly drawn to the AI field, which includes computer vision, generative modeling, and NLP. I firmly believe that artificial intelligence has the potential to transform the world for the better, provided it is used ethically. Currently, I am employed as an AI Developer at Dah Reply AI, and I am actively seeking full-time opportunities in analytics and machine learning in 2024.\n          \n            Experience            \n            \n                \n                    October 2023 – Present\n                    \n                        AI Developer\n                        \n                        Dah Reply AI (Formerly Renaissance AI)\n                    \n                \n            \n\n            Education\n            \n                \n                    September 2019 - March 2023\n                    \n                        Bachelor of Science in Physics\n                        \n                        Faculty of Science, University of Malaya\n                        \n                            Passed with Honour\n                            CGPA: 3.68\n                        \n                    \n                \n                \n                    September 2018 - June 2019\n                    \n                        Pre-University Foundation Studies - Physical Science\n                        \n                        Centre for Foundation Studies, University of Malaya\n                        \n                            Chemistry, Advanced Mathematics, Mathematics, Physics, Programming\n                            CGPA: 4.00\n                        \n                    \n                \n            \n\n            Skills\n            \n                \n                    \n                        \n                    \n                    Python Programming Language\n                \n                \n                    \n                        \n                    \n                    C Programming Language\n                     \n                \n                    \n                        \n                    \n                    R Programming Language\n                  \n                \n                    \n                        \n                    \n                    MySQL\n                  \n                \n                    \n                        \n                    \n                    Git\n                \n                \n                    \n                        \n                    \n                    Hugging Face\n                 \n                \n                    \n                        \n                    \n                    WandB\n                                                 \n                \n                    \n                        \n                    \n                    AWS\n                \n                \n                    \n                        \n                    \n                    Microsoft Azure\n                 \n                \n                    \n                        \n                    \n                    Microsoft Power BI\n                 \n                \n                    \n                        \n                    \n                    QGIS\n                 \n            \n\n            Open Source Contributions\n            \n                \n                \n                    One of the contributors for the first Malaysian Large Language Model (LLM) development by Mesolitica.\n                    Malaysia AI - Prepared training dataset with Malaysian context for Malaysian first Large Language Model (LLM) development.\n                \n            \n\n            Courses and Certificates\n            \n                \n                    March 2024\n                    \n                        K-Youth Development Programme\n                        Excelerate Asia\n                    \n                \n                \n                    March 2024\n                    \n                        Data Science Bootcamp\n                        \n                        General Assembly\n                        \n                            Link\n                        \n                    \n                \n                \n                    November 2023\n                    \n                        IBM AI Engineering Professional Certificate\n                        \n                        IBM\n                        \n                            Link\n                        \n                    \n                \n                \n                    August 2023\n                    \n                        IBM Data Science Professional Certificate\n                        \n                        IBM\n                        \n                            Link\n                                            \n                    \n                \n                \n                    August 2023\n                    \n                        KPMG AU Data Analytics Virtual Internship\n                        \n                        KPMG\n                        \n                            Link\n                        \n                    \n                \n            \n\n            Interests\n            In addition to my work as an AI Developer, I have a passion for astrophotography. I own a computerized telescope that is specifically designed for astrophotography. I capture images of deep sky objects, planets, and galaxies. I share these images on my Facebook page. I also enjoy participating in astronomy outreach programs. Recently, I had the opportunity to serve as a telescope operator for the Kuala Kubu Bharu Starfest 2023, which was organized by Majlis Daerah Hulu Selangor in collaboration with Sahabat Langit Utara."
  },
  {
    "objectID": "blog/231118_dataleakage/index.html",
    "href": "blog/231118_dataleakage/index.html",
    "title": "How I Accidentally Leaked Data in My First Machine Learning Project",
    "section": "",
    "text": "In the early stages of my journey into machine learning, I launched my first project, which was centered around classifying breast cancer. The project initially seemed successful, as I was able to achieve high accuracy during the validation phase. However, I noticed a significant drop when testing with a separate unseen dataset. It was for two months until I realized I had unintentionally leaked training data into the validation set due to improper scaling and transformation practices.\nIn this article, I will be demonstrating the common novice mistake that leads to data leakage. The mistakes I show here are exactly what I did before. To start, data leakage generally occurs when our training data is fed with information about the target, but similar data is available when the model is used in predictions. This leads to high score on the training set, but the model will perform poorly when tested with unseen data. In simple words, data leakage makes a machine learning model look very accurate until we start making predictions with new set of data to the model, and then the model becomes very inaccurate.\n\nTrain-Test Contamination\nThere are various types of data leakage, and the type I am addressing here is train-test contamination. This kind of leakage happens when the user fails to carefully distinguish between training data and validation data. For instance, during preprocessing tasks such as imputing missing values or data scaling before using train_test_split(). While the model constructed may yield a high validation score, instilling confidence, it ultimately performs poorly when tested with unseen data.\nThe dataset that I am using here is obtained from Kaggle competition dataset. In this demonstration, we will need to predict whether a software defects or not based on the features given.\n# Import required libraries and packages\nimport numpy as np                          # For linear algebra\nimport pandas as pd                         # For data manipulation\nimport matplotlib as mlt                    # For visualization\nimport matplotlib.pyplot as plt             # For visualization(scripting layer)\nimport seaborn as sns                       # For visualization\n\n# Import the data\ndf = pd.read_csv(r'/kaggle/input/playground-series-s3e23/train.csv', index_col = 'id')\n\n# Show the header of the data\ndf.head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nloc\nv(g)\nev(g)\niv(g)\nn\nv\nl\nd\ni\ne\nb\nt\nlOCode\nlOComment\nlOBlank\nlocCodeAndComment\nuniq_Op\nuniq_Opnd\ntotal_Op\ntotal_Opnd\nbranchCount\ndefects\n\n\n\n\n25.0\n6.0\n5.0\n6.0\n88.0\n461.82\n0.06\n16.92\n26.42\n7621.43\n0.15\n423.41\n19\n0\n4\n0\n18.0\n18.0\n54.0\n36.0\n11.0\nFalse\n\n\n36.0\n2.0\n1.0\n2.0\n133.0\n676.63\n0.03\n30.23\n22.23\n19091.41\n0.23\n1060.96\n27\n3\n2\n1\n16.0\n13.0\n74.0\n49.0\n3.0\nFalse\n\n\n7.0\n1.0\n1.0\n1.0\n16.0\n62.51\n0.40\n2.50\n21.59\n220.18\n0.02\n12.23\n4\n0\n1\n0\n5.0\n6.0\n11.0\n6.0\n1.0\nFalse\n\n\n22.0\n2.0\n1.0\n1.0\n94.0\n456.65\n0.09\n11.74\n39.72\n5421.87\n0.15\n301.22\n14\n0\n3\n0\n14.0\n23.0\n56.0\n36.0\n3.0\nFalse\n\n\n38.0\n5.0\n1.0\n4.0\n130.0\n644.05\n0.04\n25.91\n23.55\n15572.12\n0.21\n865.12\n22\n7\n4\n0\n15.0\n17.0\n74.0\n51.0\n9.0\nTrue\n\n\n\nFor demonstration purposes, I skipped much of the data preparation work. Now, in this dataset, I aim to log-transform the data since most features are right-skewed. However, here’s where the mistake occurred: in my attempt to perform the log-transform, I applied the transformation to the entire dataset, df.\n# Choose all the numerical columns\nnum_cols = [col for col in df.columns if col not in [\"defects\"]]\ndf_num = df[num_cols]\n\n# Apply log transformation to the numerical columns\ndf_num_transformed = np.log1p(df_num)\n\n# Concatenate the transformed numerical columns with the \"defects\" column\ndf_transformed = pd.concat([df_num_transformed, df[\"defects\"]], axis=1)\n\ndf_transformed.head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nloc\nv(g)\nev(g)\niv(g)\nn\nv\nl\nd\ni\ne\nb\nt\nlOCode\nlOComment\nlOBlank\nlocCodeAndComment\nuniq_Op\nuniq_Opnd\ntotal_Op\ntotal_Opnd\nbranchCount\ndefects\n\n\n\n\n3.258097\n1.94591\n1.79176\n1.94591\n4.48864\n6.13734\n0.05827\n2.88592\n3.31127\n8.93885\n0.13976\n6.05070\n2.99573\n0.00000\n1.60944\n0.00000\n2.94444\n2.94444\n4.00733\n3.61092\n2.48491\nFalse\n\n\n3.61092\n1.09861\n0.69315\n1.09861\n4.89784\n6.51860\n0.02956\n3.44138\n3.14545\n9.85705\n0.20701\n6.96787\n3.33221\n1.38629\n1.09861\n0.69315\n2.83321\n2.63906\n4.31749\n3.91202\n1.38629\nFalse\n\n\n2.07944\n0.69315\n0.69315\n0.69315\n2.83321\n4.15120\n0.33647\n1.25276\n3.11751\n5.39898\n0.01980\n2.58249\n1.60944\n0.00000\n0.69315\n0.00000\n1.79176\n1.94591\n2.48491\n1.94591\n0.69315\nFalse\n\n\n3.13549\n1.09861\n0.69315\n0.69315\n4.55388\n6.12610\n0.08618\n2.54475\n3.70672\n8.59838\n0.13976\n5.71116\n2.70805\n0.00000\n1.38629\n0.00000\n2.70805\n3.17805\n4.04305\n3.61092\n1.38629\nFalse\n\n\n3.66356\n1.79176\n0.69315\n1.60944\n4.87520\n6.46933\n0.03922\n3.29250\n3.20071\n9.65330\n0.19062\n6.76402\n3.13549\n2.07944\n1.60944\n0.00000\n2.77259\n2.89037\n4.31749\n3.95124\n2.30259\nTrue\n\n\n\nAs we can observe, the entire dataset has now been log-transformed. Now, let’s proceed to develop our classification model using logistic regression.\n# Import libraries and packages\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n# Make empty dictionaries to store accuracy\nval_accuracy_dict = {}\ntest_accuracy_dict = {}\n\n# Separate the data into predictor (X) and target (Y)\nX = df_transformed.drop('defects',axis=1)\nY = df_transformed['defects'].values\n \n# Split the dataset into training and validation set\nx_train1, x_val1, y_train1, y_val1 = train_test_split(X, Y, stratify=Y, test_size=0.10, random_state=45)\n\n# Initialize and train the logistic regression model. I include 1 to indicate it is Case 1/Model 1\nmodel1 = LogisticRegression()\nmodel1.fit(x_train1, y_train1)\n\n# Make predictions on the validation set\ny_pred1 = model1.predict(x_val1)\n\n# Calculate accuracy\naccuracy1 = accuracy_score(y_val1, y_pred1)\n\n# Print and store the accuracy in a dictionary\nprint(\"Accuracy:\", accuracy1)\nval_accuracy_dict['model 1'] = accuracy1\nAccuracy: 0.8136592556197028\nIn Case 1, where we transformed the entire dataset, we achieved a validation accuracy of 81%. In practice, it is advisable to perform cross-validation to ensure the reliability of our accuracy score, although we will skip this step for now. We can anticipate obtaining a similar accuracy of around 81% when using this model to predict unseen data.\nNow, let’s proceed to Case 2, where we followed good practices in transforming the dataset.\n# Separate the data into predictor (X) and target (Y)\nX = df.drop('defects',axis=1)\nY = df['defects']\n\n# Split the dataset into training and validation set. I include 2 to indicate it is Case 2/Model 2\nx_train2, x_val2, y_train2, y_val2 = train_test_split(X, Y, stratify=Y, test_size=0.10, random_state=45)\n\n# Choose all the numerical columns\nnum_cols2 = [col for col in x_train2.columns]\nnumerical_columns2 = x_train2[num_cols2] # Notice here I only select the training set instead of the whole dataset df\n\n# Apply log transformation to the numerical columns\nx_train2 = np.log1p(numerical_columns2)\n\n# Initialize and train the logistic regression model\nmodel2 = LogisticRegression()\nmodel2.fit(x_train2, y_train2)\n\n# Make predictions on the validation set\ny_pred2 = model2.predict(x_val2)\n\n# Calculate accuracy\naccuracy2 = accuracy_score(y_val2, y_pred2)\n\n# Print and store the accuracy in a dictionary\nprint(\"Accuracy:\", accuracy2)\nval_accuracy_dict['model 2'] = accuracy2\nAccuracy: 0.7716496744871637\nOh, what a surprise! Our “good practice” model performed worse on the validation set. Shocking, right? Here, we can observe that the validation accuracy of Model 2 is lower compared to the validation accuracy of Model 1. Let’s now evaluate the performance of both Model 1 and Model 2 with unseen data.\nCase 1\n# Use Model 1 to predict unseen data\npred1 = model1.predict(x_test)\n\n# Print the test accuracy score\nprint(\"Accuracy:\", accuracy_score(y_test, pred1))\ntest_accuracy_dict['model 1'] = accuracy_score(y_test, pred1)\nAccuracy: 0.768387952635975\nCase 2\n# Use Model 2 to predict unseen data\npred2 = model2.predict(x_test)\n\n# Print the test accuracy score\nprint(\"Accuracy:\", accuracy_score(y_test, pred2))\ntest_accuracy_dict['model 2'] = accuracy_score(y_test, pred2)\nAccuracy: 0.768387952635975\n\n\n\nValidation vs Test Accuracy for Model 1 and Model 2.\n\n\nThe test accuracy for both of our models appears to be the same. However, it’s crucial to note the difference in accuracy for Model 1 between the validation set and the test set. There is no difference in Model 2, which adhered to the good practice of transforming the training set only.\nThis discrepancy highlights the impact of data leakage. When data leakage occurs, the accuracy obtained during the model’s development phase tends to be overly optimistic, resulting in a high score during evaluation. However, when the model is used to predict unseen data, its performance is notably worse, similar to the scenario in Case 1.\nDuring model development, the score obtained should ideally reflect what can be expected when predicting unseen data. Experiencing a drop in accuracy during deployment, as seen in Case 1, is problematic, particularly when the model is intended for business purposes. This underscores the importance of correct data handling and following good practices, as demonstrated by Model 2, to ensure the model’s reliability in real-world applications.\nWe can enhance the practice of Model 2 by implementing a Pipeline, a topic I will explore into later. For now, I aim to illustrate the occurrence of data leakage, a rookie mistake I made during my early days in machine learning."
  },
  {
    "objectID": "blog/240214_gan-math/index.html",
    "href": "blog/240214_gan-math/index.html",
    "title": "Mathematics of GAN",
    "section": "",
    "text": "In this post, we will explore the mathematics behind GANs. This serves as an introductory post before I demonstrate and engage in practical work on using DCGAN to generate synthetic images of deep space objects in the next post. The intention behind this post is to deepen my understanding of GANs and their mathematical framework, which greatly interests me. It is one of the reasons why I love AI, due to its extensive mathematical applications."
  },
  {
    "objectID": "blog/240214_gan-math/index.html#loss-function",
    "href": "blog/240214_gan-math/index.html#loss-function",
    "title": "Mathematics of GAN",
    "section": "Loss Function",
    "text": "Loss Function\nGAN consists of two distinct models: a generative model (G) that captures the data distribution and a discriminative model (D) that estimates the probability that a sample came from the training data rather than from G. The generator produces images that resemble the training images or “fake” images. During training, the generator continuously tries to outsmart the discriminator by generating better and better fakes, while the discriminator strives to become a better detective and accurately classify real and fake images. As discriminator learns to discriminate between the two, it provides feedback to the generator about the quality of its generated samples. This is why the term “adversarial” is used here. Equilibrium is achieved when the generator produces perfect fakes that closely resemble samples from the training data, and the discriminator is left guessing at a 50% confidence level whether the generator output is real. Thus, there will be a loss function to evaluate the performance of each of the models.\n\n\n\nIllustration of GAN training. Credit image: blog.ovhcloud.com.\n\n\n\nNotation\nDefining notation that we will be using in this post: \\[\n\\begin{aligned}\n& x \\ \\text{: Real data} \\\\\n& z \\ \\text{: Latent space vector sampled from a standard normal distribution} \\\\\n& G(z) \\ \\text{: Fake data} \\\\\n& D(x) \\ \\text{: Discriminator's evaluation of real data} \\\\\n& D(G(x)) \\ \\text{: Discriminator's evaluation of fake data} \\\\\n& \\text{Error}(a,b) \\ \\text{: Error between a and b} \\\\\n\\end{aligned}\n\\]\n\n\nBinary Cross Entropy\nA common loss function that is used in binary classification problems is binary cross entropy. This mathematically described as: \\[\n\\begin{aligned}\n& H(p,q) = \\mathbb{E}_{x \\sim p_{(x)}}[- \\log q (x)]\n\\end{aligned}\n\\tag{1}\\]\nFor a classification task, the random variable is discrete. Therefore, the Equation 1 can be expressed as a summation: \\[\n\\begin{aligned}\n& H(p,q) = -\\sum_{x \\in \\chi}p(x)\\log q (x)\n\\end{aligned}\n\\tag{2}\\]\nIn the context of binary classification, we have two possible labels: 0 and 1. Therefore, the sum in the above expression would involve only two terms, corresponding to these two labels.\nLet’s denote the true label as \\(y\\) and the predicted probability for class 1 as \\(\\hat{y}\\). Thus, we have:\n\nFor a real input, \\(y = 1\\). Substituting this into the binary cross-entropy loss function gives the following loss value:\n\n\\[\n\\begin{aligned}\n& H(y,\\hat{y})  = - (y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}))\n\\\\\n& H(1,\\hat{y})  = - y \\log(\\hat{y})\n\\end{aligned}\n\\tag{3}\\]\n\nSimilarly, for a fake input, \\(y = 0\\). Substituting this into the binary cross-entropy loss function gives the following loss value:\n\n\\[\n\\begin{aligned}\n& H(y,\\hat{y})  = - (y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})) \\\\\n& H(0,\\hat{y})  = (1 - y) \\log(1 - \\hat{y})\n\\end{aligned}\n\\tag{4}\\]\nTherefore, we can rewrite the expression for the binary cross-entropy loss as:\n\\[\n\\begin{aligned}\n& H(y,\\hat{y}) = -\\sum y\\log (\\hat{y}) + (1-y)\\log (1-\\hat{y})\n\\end{aligned}\n\\]\n\n\nThe Discriminator\nThe discriminator task is to correctly label generated images as false and empirical data points as true. We might consider the loss function of the discriminator: \\[\n\\begin{aligned}\n& L_{D} = \\text{Error}(D(x),1) + \\text{Error}(D(G(z)),0)\n\\end{aligned}\n\\tag{5}\\]\nApplying Equation 2 into Equation 5, the expression will be written as:\n\\[\n\\begin{aligned}\n& L_{D} = -\\sum_{x \\in \\chi, z \\in \\zeta} \\log(D(x)) + \\log(1-D(G(z)))\n\\end{aligned}\n\\tag{6}\\]\n\n\nThe Generator\nThe generator task is to confuse the discriminator as much as possible such that it mislabels generated images as being true. The loss function for the generator is written as:\n\\[\n\\begin{aligned}\n& L_{G} = \\text{Error}(D(G(z)),1)\n\\end{aligned}\n\\tag{7}\\]\nBecause loss function is something we want to minimize, in the case of the generator, it should minimize the difference between, the label for true data and the discriminator’s evaluation of the generated fake data.\nApplying Equation 2 into Equation 7, the expression will be written as:\n\\[\n\\begin{aligned}\n& L_{G} = -\\sum_{x \\in \\zeta} \\log(D(G(z)))\n\\end{aligned}\n\\tag{8}\\]\nNow, we have two loss functions to train both the generator and the discriminator, which work together. If \\(D(G(z))\\) is close to 1 when the discriminator evaluates the generator’s output, then the generator’s loss is small. This occurs because a discriminator output of 1 indicates that the discriminator is highly confident that the generated image is real, hence the generator’s loss is minimized. This aligns with our objective for the generator’s loss function."
  },
  {
    "objectID": "blog/240214_gan-math/index.html#model-optimization",
    "href": "blog/240214_gan-math/index.html#model-optimization",
    "title": "Mathematics of GAN",
    "section": "Model Optimization",
    "text": "Model Optimization\nModel optimization is the process of using mathematical and computational methods to solve optimization problems. For our case, the goal is to find parameters for both the generator and the discriminator such that the loss functions are optimized. In practical terms, this involves training the models and minimizing the loss function.\n\nTraining GAN\nIn the training of a GAN, we aim to optimize both the generator \\(G\\) and the discriminator \\(D\\) simultaneously. To do this, we need to combine the individual loss functions of the generator and discriminator into a single objective function that captures the overall performance of the GAN. Combining both the loss functions Equation 6 and Equation 8 will give us a objective function which is defined as a function of \\(G\\) and \\(D\\). This objective function is expressed as:\n\\[\n\\begin{aligned}\n& V(G, D) = \\mathbb{E}_{x \\sim p_{\\text{data}}}[ \\log(D(x))] + \\mathbb{E}_{z \\sim p_z}[\\log(1 - D(G(z)))]\n\\end{aligned}\n\\tag{9}\\]\nThis objective function measures the effectiveness of the discriminator in distinguishing real data from generated data produced by the generator. Here, \\(x\\) represents real data samples drawn from the true data distribution \\(p_{data}\\) and \\(z\\) represents random noise vectors sampled from a distribution \\(p_{z}\\).\nBecause we are interested in the distribution that is modeled by the generator, we will rewrite the Equation 9 by defining a new variable \\(y=G(z)\\). This makes it easier to understand that the GAN objective function is terms of the generated samples rather than the latent vector: \\[\n\\begin{aligned}\n& V(G, D) = \\mathbb{E}_{x \\sim p_{\\text{data}}}[\\log(D(x))] + \\mathbb{E}_{y \\sim p_g}[\\log(1 - D(y))]\n\\end{aligned}\n\\tag{10}\\]\nIn cases where the dimensionality of the output space \\((y)\\) is higher than the dimensionality of the input space \\((z)\\), we can represent Equation 10 as an integral over the input space:\n\\[\n\\begin{aligned}\n& V(G, D) = \\int_{x \\in \\chi} p_{\\text{data}}(x) \\log(D(x)) + p_g(x) \\log(1 - D(x)) \\, dx\n\\end{aligned}\n\\]\nIn the Equation 10, the first term represents the expected value of the logarithm of the discriminator’s output when fed with real data \\(x\\) sampled from the true data distribution \\(p_{\\text{data}}(x)\\) and the second term represents the expected value of the logarithm of \\(\\log(1 - D(x))\\) when \\(x\\) is sampled from the generator’s distribution \\(p_g(x)\\) (Read more about Expectation Value).\nThe optimization of the generator and discriminator involves a minimax game, where the generator tries to minimize its loss while the discriminator tries to maximize its own. In other words:\n\\[\n\\begin{aligned}\n& \\min_G \\max_D V(D, G)\n\\end{aligned}\n\\tag{11}\\]\n\n\nTraining Discriminator\nThe task of the discriminator is to maximize this objective function. Remember, the discriminator’s role in the GAN framework is to accurately differentiate between real and generated samples. By maximizing \\(V(G,D)\\), we ensure that the discriminator is trained to assign high probabilities to real samples and low probabilities to generated samples. This encourages the generator to produce samples that are increasingly similar to real data, ultimately improving the quality of the generated output. We can do the partial derivative of \\(V(G,D)\\) with respect to \\(D(x)\\) to find the optimal value of the discriminator denoted as \\(D_{Optimal}(x)\\). The result of this derivative yields the expression: \\[\n\\begin{aligned}\nD_{\\text{Optimal}}(x) = \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}\n\\end{aligned}\n\\tag{12}\\]\nThis equation represents the optimal discriminator for a given input \\(x\\). It calculates the probability that the input \\(x\\) belongs to the real data distribution \\(p_{\\text{data}}(x)\\) relative to the sum of probabilities from both the real and generated data distributions.\nTo explain the Equation 12 intuitively, \\(D_{\\text{Optimal}}(x)\\) assigns a value close to 1 when the sample \\(x\\) is highly likely to come from the real data distribution, and a value close to 0 when it is more likely to come from the generator’s distribution. This ensures that the discriminator effectively distinguishes between real and generated samples.\n\n\nTraining Generator\nTraining the generator, we assume the discriminator to be fixed. Substitute the equation Equation 12 into our objective function Equation 9: \\[\n\\begin{aligned}\n& V(G, D_{\\text{Optimal}}) = \\mathbb{E}_{x \\sim p_{\\text{data}}}[\\log(D_{\\text{Optimal}}(x))] + \\mathbb{E}_{y \\sim p_g}[\\log(1 - D_{\\text{Optimal}}(x))] \\\\\n& V(G, D_{\\text{Optimal}}) = \\mathbb{E}_{x \\sim p_{\\text{data}}}[\\log{\\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}}] + \\mathbb{E}_{x \\sim p_{\\text{g}}}[\\log{\\frac{p_{\\text{g}}(x)}{p_{\\text{data}}(x) + p_g(x)}}]\n\\end{aligned}\n\\tag{13}\\]\nThe equation eventually is expressed as a Jensen-Shannon divergence: \\[\n\\begin{aligned}\n& V(G,D^*) = -\\log 4 + 2 \\cdot D_{JS}(p_{\\text{data}} \\| p_g)\n\\end{aligned}\n\\tag{14}\\]\nThis occurs through a process after the substitution with clever tricks to interpret the expectations as Kullback-Leibler divergence, derived from the properties of logarithms in the equation.\nFinally, we can interpret Equation 14 as follows: during training of the generator, the goal is to minimize the objective function \\(V(G,D)\\) such that we want the Jensen-Shannon divergence between the distribution of the data and the distribution of the generated samples to be as small as possible. The part here, \\(p_{\\text{data}} \\| p_g\\), implies that the distributions \\(p_{g}\\) and \\(p_{data}\\) should be as close to each other as possible. Therefore, the optimal generator \\(G\\) is the one that able to mimic \\(p_{data}\\) from the generated sample distribution \\(p_{g}\\)."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "NeuralBites",
    "section": "",
    "text": "Welcome to my blog NeuralBites. I am excited that you are here! You will find a collection of articles on a wide variety of topics. As a learner, I want to share with you what I pick up along the way. I hope you will find something of interest in my writing. So, take a look around and let me know what you think."
  },
  {
    "objectID": "blog/index.html#section",
    "href": "blog/index.html#section",
    "title": "NeuralBites",
    "section": "2024",
    "text": "2024\n\n\n    \n                  \n             February 18, 2024\n        \n        \n            AI Learns to Generate Synthetic Deep Space Images\n            \n                \n                    data science\n                \n                \n                    project\n                \n            \n            Training AI with DCGAN to generate synthetic deep space images.\n        \n        \n            \n        \n    \n    \n                  \n             February 18, 2024\n        \n        \n            Mathematics of GAN\n            \n                \n                    data science\n                \n                \n                    theoretical\n                \n            \n            Discover the mathematics behind the beauty of GAN.\n        \n        \n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-1",
    "href": "blog/index.html#section-1",
    "title": "NeuralBites",
    "section": "2023",
    "text": "2023\n\n\n    \n                  \n             November 19, 2023\n        \n        \n            How I Accidentally Leaked Data in My First Machine Learning Project\n            \n                \n                    data science\n                \n            \n            Exploring into my early days in machine learning as I am sharing a rookie mistake that led to data leakage. Learn firsthand how data leakage happens.\n        \n        \n            \n        \n    \n    \n                  \n             November 18, 2023\n        \n        \n            Hello World!\n            \n                \n                    news\n                \n            \n            Over the past few months, my homepage served as a placeholder for my porfolio. As I have interest in learning and sharing knowledge, I have chosen to utilize this platform to document my data science journey. And now, here we are.\n        \n        \n            \n        \n    \n\n\nNo matching items\n\n\n\nThe blog post listing is based on the website source of Marvin Schmitt, who has put together an incredible listing template under CC-BY-SA 4.0 license. Thank you!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, I'm Hazman.",
    "section": "",
    "text": "I am a Physics graduate from University of Malaya with a strong interest for data science, AI, and machine learning.\n            In my role as an AI Developer, I leverage AI technologies to address clients' needs. My expertise includes Python programming, developing machine learning models, deploying models using AWS, and proficiently managing AI projects.\n            \n            JOB MARKET 2024: I am looking for machine learning positions in Malaysia starting in 2024. My current interests revolve around deep learning, computer vision, NLP and also deploying these AI technologies for applications. I am also open to learn new topics. I spend a considerable amount of my time programming in Python (PyTorch + TensorFlow). \n            Here is my CV (Link). Reach out if you think there might be a fit: hazmannaimbinahsan@gmail.com"
  },
  {
    "objectID": "index.html#current-updates",
    "href": "index.html#current-updates",
    "title": "Hi, I'm Hazman.",
    "section": "Current Updates",
    "text": "Current Updates\n\nOctober 23, 2023: Great news! I have begun my on-the-job training as an AI Developer, which is my first baby-step for my data science career. I am part of the AI Backend team that works on developing AI-based applications for E-commerce. I also get to work with other areas of AI such as NLP and LLM. This training will go on for three months.\nSeptember 9, 2023: I have been selected for the K-Youth Development Programme, a data science upskilling program offered by Excelerate Asia in partnership with General Assembly. This program is sponsored by Khazanah Nasional and lasts for four months. The first month is dedicated to learning data science skills through a bootcamp, while the remaining three months involve on-the-job training. The program also helps me develop career skills such as resume writing, interviewing, and communication.\nJuly 7, 2023: I have recently joined Kaggle, the world’s largest data science community. This platform provides me with the opportunity to learn from and collaborate with other data scientists, as well as participate in exciting machine learning competitions. I am eager to explore all that Kaggle has to offer and continue to grow in my data science journey. Join me on Kaggle and let’s go explore our data science journey together.\nJune 10, 2023: I made a career choice to pursue data science. Since I had a physics background and not a computer science one, I began to look for resources that would help me start my journey in data science. I enrolled in the IBM Data Science Professional Certificate program on Coursera which would enable me to improve my skills and knowledge in this field."
  },
  {
    "objectID": "index.html#featured-blog-posts",
    "href": "index.html#featured-blog-posts",
    "title": "Hi, I'm Hazman.",
    "section": "Featured Blog Posts",
    "text": "Featured Blog Posts\n\n\n\n\n\n\n\n\n\n\nMathematics of GAN\n\n\n\nFeb 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow I Accidentally Leaked Data in My First Machine Learning Project\n\n\n\nNov 19, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  }
]