[
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Click to make it larger.\n\n\n|2023-09-01|Code|\nThis project aims to develop a machine learning model for categorizing galaxies, stars, and quasars using SDSS data. Similar to other cosmological surveys, SDSS contributes an exceptional volume of data generated by the telescope and instruments. Notably, the telescope produced hundreds of gigabytes of raw data every night, a scale that was previously unprecedented. The manual classification of each object requires a significant amount of labor and proves to be inefficient in terms of both cost and time. To simplify this resource-intensive process, the application of machine learning emerges as a solution to enhance efficiency in astronomical research. This highlights the valuable utility of machine learning, which excels in deciphering intricate patterns within large datasets, fundamentally transforming the landscape of the astronomical domain.\nFor this project, I am utilizing a dataset from the Sloan Digital Sky Survey (SDSS) sky survey. SDSS is a major multi-spectral imaging and spectroscopic redshift survey conducted using a dedicated 2.5-m wide-angle optical telescope. Millions of sources have already been cataloged by SDSS, and the next generation of telescopes is predicted to significantly increase the size of source catalogs. With such large datasets, machine learning algorithms are becoming increasingly valuable tools for analysis and data exploration, enhancing data processing efficiency and reducing resource-intensive tasks.\nI developed multiple models, including logistic regression, support vector classifier, and random forest classifier. Among these models, the random forest classifier achieved the highest accuracy, successfully classifying galaxies, stars, and quasars with ~90% accuracy. In this project, essential pre-processing steps were applied to the dataset, such as data scaling to ensure standardization. Additionally, I implemented 5-fold cross-validation to reliably assess the models’ performance and conducted hyperparameter tuning to optimize their effectiveness.\n\n\n\n      \n\n\n\n\n\n\nClick to make it larger.\n\n\n|2023-08-23|Code|\nMy project focused on developing a machine learning model to predict house sales prices in King County. The primary objective of this project is to provide accurate price predictions that empower various stakeholders, including homebuyers, sellers, and real estate professionals, to make well-informed decisions. By analyzing property attributes and historical sales data, the machine learning model aims to uncover the key factors influencing house prices, offer insights into market trends, and contribute to equitable real estate transactions.\nTo build these models, I employed a variety of machine learning techniques, including linear regression, multiple linear regression, support vector regression, and the random forest regressor provided by scikit-learn. Pre-processing of the dataset was a crucial step, and I ensured its reliability by utilizing 5-fold cross-validation. Additionally, I fine-tuned the models through hyperparameter tuning to optimize their performance.\nThe outcomes of the project are notable. I successfully developed and evaluated 17 distinct machine learning models using various algorithms, such as Ordinary Least Squares, Ridge Regression, Lasso, Multiple Linear Regression, Support Vector Regression, and Random Forest. Through 5-fold cross-validation, I assessed the predictive capabilities of these models and gained insights into their effectiveness. The project underscores the importance of selecting models that align with the dataset while striking a balance between feature adjustments, tuning, and complexity.\n\n\n\n    \n\n \n\n\n\n\n\n\n\nClick to make it larger.\n\n\n|2023-07-08|Code|\nThis data analysis project aims to provide a comprehensive analysis and visualization of seat distribution in electoral systems based on margin-of-votes classification in two Malaysian general elections, namely General Election 14 and General Election 15. The project utilizes data scraping, data preprocessing, and visualization techniques to gain insights into how different alliances perform in elections and how seats are distributed among them based on voting margins. This information can be valuable for political analysts, policymakers, and individuals interested in understanding the dynamics of elections and their outcomes."
  },
  {
    "objectID": "projects/index.html#differentiating-galaxies-quasars-and-stars-through-utilization-of-various-classification-algorithms-with-sdss-sky-survey-data",
    "href": "projects/index.html#differentiating-galaxies-quasars-and-stars-through-utilization-of-various-classification-algorithms-with-sdss-sky-survey-data",
    "title": "Projects",
    "section": "",
    "text": "Click to make it larger.\n\n\n|2023-09-01|Code|\nThis project aims to develop a machine learning model for categorizing galaxies, stars, and quasars using SDSS data. Similar to other cosmological surveys, SDSS contributes an exceptional volume of data generated by the telescope and instruments. Notably, the telescope produced hundreds of gigabytes of raw data every night, a scale that was previously unprecedented. The manual classification of each object requires a significant amount of labor and proves to be inefficient in terms of both cost and time. To simplify this resource-intensive process, the application of machine learning emerges as a solution to enhance efficiency in astronomical research. This highlights the valuable utility of machine learning, which excels in deciphering intricate patterns within large datasets, fundamentally transforming the landscape of the astronomical domain.\nFor this project, I am utilizing a dataset from the Sloan Digital Sky Survey (SDSS) sky survey. SDSS is a major multi-spectral imaging and spectroscopic redshift survey conducted using a dedicated 2.5-m wide-angle optical telescope. Millions of sources have already been cataloged by SDSS, and the next generation of telescopes is predicted to significantly increase the size of source catalogs. With such large datasets, machine learning algorithms are becoming increasingly valuable tools for analysis and data exploration, enhancing data processing efficiency and reducing resource-intensive tasks.\nI developed multiple models, including logistic regression, support vector classifier, and random forest classifier. Among these models, the random forest classifier achieved the highest accuracy, successfully classifying galaxies, stars, and quasars with ~90% accuracy. In this project, essential pre-processing steps were applied to the dataset, such as data scaling to ensure standardization. Additionally, I implemented 5-fold cross-validation to reliably assess the models’ performance and conducted hyperparameter tuning to optimize their effectiveness."
  },
  {
    "objectID": "projects/index.html#modeling-and-predicting-housing-prices-in-king-county-usa-analyzing-house-sales-data",
    "href": "projects/index.html#modeling-and-predicting-housing-prices-in-king-county-usa-analyzing-house-sales-data",
    "title": "Projects",
    "section": "",
    "text": "Click to make it larger.\n\n\n|2023-08-23|Code|\nMy project focused on developing a machine learning model to predict house sales prices in King County. The primary objective of this project is to provide accurate price predictions that empower various stakeholders, including homebuyers, sellers, and real estate professionals, to make well-informed decisions. By analyzing property attributes and historical sales data, the machine learning model aims to uncover the key factors influencing house prices, offer insights into market trends, and contribute to equitable real estate transactions.\nTo build these models, I employed a variety of machine learning techniques, including linear regression, multiple linear regression, support vector regression, and the random forest regressor provided by scikit-learn. Pre-processing of the dataset was a crucial step, and I ensured its reliability by utilizing 5-fold cross-validation. Additionally, I fine-tuned the models through hyperparameter tuning to optimize their performance.\nThe outcomes of the project are notable. I successfully developed and evaluated 17 distinct machine learning models using various algorithms, such as Ordinary Least Squares, Ridge Regression, Lasso, Multiple Linear Regression, Support Vector Regression, and Random Forest. Through 5-fold cross-validation, I assessed the predictive capabilities of these models and gained insights into their effectiveness. The project underscores the importance of selecting models that align with the dataset while striking a balance between feature adjustments, tuning, and complexity."
  },
  {
    "objectID": "projects/index.html#electoral-analysis-and-visualization-understanding-seat-distribution-by-margin-of-votes-classification",
    "href": "projects/index.html#electoral-analysis-and-visualization-understanding-seat-distribution-by-margin-of-votes-classification",
    "title": "Projects",
    "section": "",
    "text": "Click to make it larger.\n\n\n|2023-07-08|Code|\nThis data analysis project aims to provide a comprehensive analysis and visualization of seat distribution in electoral systems based on margin-of-votes classification in two Malaysian general elections, namely General Election 14 and General Election 15. The project utilizes data scraping, data preprocessing, and visualization techniques to gain insights into how different alliances perform in elections and how seats are distributed among them based on voting margins. This information can be valuable for political analysts, policymakers, and individuals interested in understanding the dynamics of elections and their outcomes."
  },
  {
    "objectID": "projects/index.html#studies-on-bipolar-outflows-in-star-forming-region-afgl5142-with-alma",
    "href": "projects/index.html#studies-on-bipolar-outflows-in-star-forming-region-afgl5142-with-alma",
    "title": "Projects",
    "section": "Studies on Bipolar Outflows in Star Forming Region AFGL5142 With ALMA",
    "text": "Studies on Bipolar Outflows in Star Forming Region AFGL5142 With ALMA\n\n\nClick to make it larger.\n\n|2022-06-28|Thesis|\nAs an undergraduate student, I had the privilege of conducting research in the field of star formation. My final year project, “Studies on Bipolar Outflows in Star Forming Region AFGL 5142 with ALMA,” was supervised by a prominent Malaysian radio astronomer, Prof. Dr. Zamri Bin Zainal Abidin. In this project, I analyzed ALMA archival data to investigate the outflows from the AFGL 5142 star forming region. I utilized radio spectroscopy to observe the line emission of 13C17O and detect the bipolar outflows through redshifted and blueshifted emissions. Furthermore, I calculated the properties of the outflows such as mass, momentum, and energy, and proposed that they may have played a role in promoting star formation in neighboring regions. I processed the data using CASA, the primary software for the Atacama Large Millimeter/submillimeter Array (ALMA), and further analyzed the data using Python with Astropy and AplPy packages. I was also honored to work with professional astronomers in the Radio Cosmology Research Laboratory.My research project earned me the Best Presentation Award from the Department of Physics at University of Malaya, reflecting my dedication and hard work. My experience in this project has honed my skills and broadened my knowledge in the field of radio astronomy."
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum vitae",
    "section": "",
    "text": "Download Current Resume"
  },
  {
    "objectID": "blog/230707_datascienceastrophysics/index.html",
    "href": "blog/230707_datascienceastrophysics/index.html",
    "title": "How I Accidentally Leaked Data in My First Machine Learning Project",
    "section": "",
    "text": "In the early stages of my data science journey, I enrolled in the IBM Data Science Professional Certificate on Coursera, eager to kickstart my career. I did my first machine learning project, focused on classifying breast cancer. Despite achieving high accuracy during validation, I later discovered a significant drop when testing with a separate dataset. It was for two months until I realized I had unintentionally leaked training data into the validation set due to improper scaling and transformation practices.\nIn this article, I will be demonstrating the common novice mistake that leads to data leakage. The mistakes I show here are exactly what I did before. To start, data leakage generally occurs when our training data is fed with information about the target, but similar data is available when the model is used in predictions. This leads to high performance on the training set, but the model will perform poorly when tested with unseen data. In simple words, data leakage makes a machine learning model look very accurate until we start making predictions with new set of data to the model, and then the model becomes very inaccurate.\n\nTrain-Test Contamination\nThere are various types of data leakage, and the type I am addressing here is train-test contamination. This kind of leakage happens when the user fails to carefully distinguish between training data and validation data. For instance, during preprocessing tasks such as imputing missing values or data scaling before using train_test_split(). While the model constructed may yield a high validation score, instilling confidence, it ultimately performs poorly when tested with unseen data.\nThe dataset that I am using here is obtained from Kaggle competition dataset. In this demonstration, we will need to predict whether a software defects or not based on the features given.\n# Import required libraries and packages\nimport numpy as np                          # For linear algebra\nimport pandas as pd                         # For data manipulation\nimport matplotlib as mlt                    # For visualization\nimport matplotlib.pyplot as plt             # For visualization(scripting layer)\nimport seaborn as sns                       # For visualization\n\n# Import the data\ndf = pd.read_csv(r'/kaggle/input/playground-series-s3e23/train.csv', index_col = 'id')\n\n# Show the header of the data\ndf.head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nloc\nv(g)\nev(g)\niv(g)\nn\nv\nl\nd\ni\ne\nb\nt\nlOCode\nlOComment\nlOBlank\nlocCodeAndComment\nuniq_Op\nuniq_Opnd\ntotal_Op\ntotal_Opnd\nbranchCount\ndefects\n\n\n\n\n25.0\n6.0\n5.0\n6.0\n88.0\n461.82\n0.06\n16.92\n26.42\n7621.43\n0.15\n423.41\n19\n0\n4\n0\n18.0\n18.0\n54.0\n36.0\n11.0\nFalse\n\n\n36.0\n2.0\n1.0\n2.0\n133.0\n676.63\n0.03\n30.23\n22.23\n19091.41\n0.23\n1060.96\n27\n3\n2\n1\n16.0\n13.0\n74.0\n49.0\n3.0\nFalse\n\n\n7.0\n1.0\n1.0\n1.0\n16.0\n62.51\n0.40\n2.50\n21.59\n220.18\n0.02\n12.23\n4\n0\n1\n0\n5.0\n6.0\n11.0\n6.0\n1.0\nFalse\n\n\n22.0\n2.0\n1.0\n1.0\n94.0\n456.65\n0.09\n11.74\n39.72\n5421.87\n0.15\n301.22\n14\n0\n3\n0\n14.0\n23.0\n56.0\n36.0\n3.0\nFalse\n\n\n38.0\n5.0\n1.0\n4.0\n130.0\n644.05\n0.04\n25.91\n23.55\n15572.12\n0.21\n865.12\n22\n7\n4\n0\n15.0\n17.0\n74.0\n51.0\n9.0\nTrue\n\n\n\nFor demonstration purposes, I skipped much of the data preparation work. Now, in this dataset, I aim to log-transform the data since most features are right-skewed. However, here’s where the mistake occurred: in my attempt to perform the log-transform, I applied the transformation to the entire dataset, df.\n# Choose all the numerical columns\nnum_cols = [col for col in df.columns if col not in [\"defects\"]]\ndf_num = df[num_cols]\n\n# Apply log transformation to the numerical columns\ndf_num_transformed = np.log1p(df_num)\n\n# Concatenate the transformed numerical columns with the \"defects\" column\ndf_transformed = pd.concat([df_num_transformed, df[\"defects\"]], axis=1)\n\ndf_transformed.head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nloc\nv(g)\nev(g)\niv(g)\nn\nv\nl\nd\ni\ne\nb\nt\nlOCode\nlOComment\nlOBlank\nlocCodeAndComment\nuniq_Op\nuniq_Opnd\ntotal_Op\ntotal_Opnd\nbranchCount\ndefects\n\n\n\n\n3.258097\n1.94591\n1.79176\n1.94591\n4.48864\n6.13734\n0.05827\n2.88592\n3.31127\n8.93885\n0.13976\n6.05070\n2.99573\n0.00000\n1.60944\n0.00000\n2.94444\n2.94444\n4.00733\n3.61092\n2.48491\nFalse\n\n\n3.61092\n1.09861\n0.69315\n1.09861\n4.89784\n6.51860\n0.02956\n3.44138\n3.14545\n9.85705\n0.20701\n6.96787\n3.33221\n1.38629\n1.09861\n0.69315\n2.83321\n2.63906\n4.31749\n3.91202\n1.38629\nFalse\n\n\n2.07944\n0.69315\n0.69315\n0.69315\n2.83321\n4.15120\n0.33647\n1.25276\n3.11751\n5.39898\n0.01980\n2.58249\n1.60944\n0.00000\n0.69315\n0.00000\n1.79176\n1.94591\n2.48491\n1.94591\n0.69315\nFalse\n\n\n3.13549\n1.09861\n0.69315\n0.69315\n4.55388\n6.12610\n0.08618\n2.54475\n3.70672\n8.59838\n0.13976\n5.71116\n2.70805\n0.00000\n1.38629\n0.00000\n2.70805\n3.17805\n4.04305\n3.61092\n1.38629\nFalse\n\n\n3.66356\n1.79176\n0.69315\n1.60944\n4.87520\n6.46933\n0.03922\n3.29250\n3.20071\n9.65330\n0.19062\n6.76402\n3.13549\n2.07944\n1.60944\n0.00000\n2.77259\n2.89037\n4.31749\n3.95124\n2.30259\nTrue\n\n\n\nAs we can observe, the entire dataset has now been log-transformed. Now, let’s proceed to develop our classification model using logistic regression.\n# Import libraries and packages\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n# Make empty dictionaries to store accuracy\nval_accuracy_dict = {}\ntest_accuracy_dict = {}\n\n# Separate the data into predictor (X) and target (Y)\nX = df_transformed.drop('defects',axis=1)\nY = df_transformed['defects'].values\n \n# Split the dataset into training and validation set\nx_train1, x_val1, y_train1, y_val1 = train_test_split(X, Y, stratify=Y, test_size=0.10, random_state=45)\n\n# Initialize and train the logistic regression model. I include 1 to indicate it is Case 1/Model 1\nmodel1 = LogisticRegression()\nmodel1.fit(x_train1, y_train1)\n\n# Make predictions on the validation set\ny_pred1 = model1.predict(x_val1)\n\n# Calculate accuracy\naccuracy1 = accuracy_score(y_val1, y_pred1)\n\n# Print and store the accuracy in a dictionary\nprint(\"Accuracy:\", accuracy1)\nval_accuracy_dict['model 1'] = accuracy1\nAccuracy: 0.8136592556197028\nIn Case 1, where we transformed the entire dataset, we achieved a validation accuracy of 81%. In practice, it is advisable to perform cross-validation to ensure the reliability of our accuracy score, although we will skip this step for now. We can anticipate obtaining a similar accuracy of around 81% when using this model to predict unseen data.\nNow, let’s proceed to Case 2, where we followed good practices in transforming the dataset.\n# Separate the data into predictor (X) and target (Y)\nX = df.drop('defects',axis=1)\nY = df['defects']\n\n# Split the dataset into training and validation set. I include 2 to indicate it is Case 2/Model 2\nx_train2, x_val2, y_train2, y_val2 = train_test_split(X, Y, stratify=Y, test_size=0.10, random_state=45)\n\n# Choose all the numerical columns\nnum_cols2 = [col for col in x_train2.columns]\nnumerical_columns2 = x_train2[num_cols2] # Notice here I only select the training set instead of the whole dataset df\n\n# Apply log transformation to the numerical columns\nx_train2 = np.log1p(numerical_columns2)\n\n# Initialize and train the logistic regression model\nmodel2 = LogisticRegression()\nmodel2.fit(x_train2, y_train2)\n\n# Make predictions on the validation set\ny_pred2 = model2.predict(x_val2)\n\n# Calculate accuracy\naccuracy2 = accuracy_score(y_val2, y_pred2)\n\n# Print and store the accuracy in a dictionary\nprint(\"Accuracy:\", accuracy2)\nval_accuracy_dict['model 2'] = accuracy2\nAccuracy: 0.7716496744871637\nOh, what a surprise! Our “good practice” model performed worse on the validation set. Shocking, right? Here, we can observe that the validation accuracy of Model 2 is lower compared to the validation accuracy of Model 1. Let’s now evaluate the performance of both Model 1 and Model 2 with unseen data.\nCase 1\n# Use Model 1 to predict unseen data\npred1 = model1.predict(x_test)\n\n# Print the test accuracy score\nprint(\"Accuracy:\", accuracy_score(y_test, pred1))\ntest_accuracy_dict['model 1'] = accuracy_score(y_test, pred1)\nAccuracy: 0.768387952635975\nCase 2\n# Use Model 2 to predict unseen data\npred2 = model2.predict(x_test)\n\n# Print the test accuracy score\nprint(\"Accuracy:\", accuracy_score(y_test, pred2))\ntest_accuracy_dict['model 2'] = accuracy_score(y_test, pred2)\nAccuracy: 0.768387952635975\n\n\n\nValidation vs Test Accuracy for Model 1 and Model 2.\n\n\nThe test accuracy for both of our models appears to be the same. However, it’s crucial to note the difference in accuracy for Model 1 between the validation set and the test set. There is no difference in Model 2, which adhered to the good practice of transforming the training set only.\nThis discrepancy highlights the impact of data leakage. When data leakage occurs, the accuracy obtained during the model’s development phase tends to be overly optimistic, resulting in a high score during evaluation. However, when the model is used to predict unseen data, its performance is notably worse, similar to the scenario in Case 1.\nDuring model development, the score obtained should ideally reflect what can be expected when predicting unseen data. Experiencing a drop in accuracy during deployment, as seen in Case 1, is problematic, particularly when the model is intended for business purposes. This underscores the importance of correct data handling and following good practices, as demonstrated by Model 2, to ensure the model’s reliability in real-world applications.\nWe can enhance the practice of Model 2 by implementing a Pipeline, a topic I will explore into later. For now, I aim to illustrate the occurrence of data leakage, a rookie mistake I made during my early days in machine learning."
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "Get to Know Me!",
    "section": "",
    "text": "About\n\n“Develop a passion for learning. If you do, you will never cease to grow.” - Anthony J. D’Angelo\n\nI have a genuine passion for learning because I believe it’s the key to personal growth. To me, knowledge is akin to a precious gem – valuable and capable of making a positive impact on the world. This mindset has turned me into a naturally curious individual, always eager to acquire and refine skills, with a particular focus on Python, data science, machine learning, and AI.\nMy background in Physics has instilled in me the importance of understanding the fundamental principles that drive algorithms. I don’t settle for simply copying and pasting code; I strive for a deeper comprehension of how things work.\n\n“The only way to grow is to challenge yourself.” - Ashley Tisdale\n\nThis quote deeply resonates with me. Building on this philosophy, I’ve come to understand that the only way to truly grow is by challenging oneself. This realization has motivated me to actively explore unfamiliar territories. I eagerly embrace new opportunities, extending beyond my comfort zone.\nIn addition to my core focus on Python, data science, machine learning, and AI, I have explored into diverse domains. For instance, I’ve ventured into natural sciences like genetics and biology. Simultaneously, I’ve explored linguistic domains, utilizing natural language processing to navigate the intricacies of language. Furthermore, I’ve not shied away from delving into business-related applications, broadening the scope of my knowledge and skills. In addition, I also participate in various data science competitions. These experiences continually push me to improve and evolve.\n\n\nExperience\nAI Developer | October 2023 - Present\nRenaissance AI\n\nSpearheaded Research and Development initiatives, driving the innovation and implementation of cutting-edge AI technologies for diverse applications.\nDeveloped a robust backend system integral to a sophisticated recommendation system for an e-commerce platform. Utilized advanced deep neural network models to generate high-fidelity image embeddings, enhancing product recommendations and user experience.\nCollaborated with cross-functional teams to analyze business requirements, translating them into scalable and efficient AI solutions that contributed to increased customer engagement and satisfaction.\n\n\n\nEducation\nUniversiti Malaya | September 2019 - March 2023\nBachelor of Science in Physics\n\nPassed with Honour\nCGPA: 3.68\n\nUniversiti Malaya | September 2018 - June 2019\nPhysical Science\n\nChemistry, Additional Mathematics, Mathematics, Physics, Programming\nCGPA: 4.00\n\n\n\nSkills\nProgramming Languages:\n\nPython\nC\nSQL\n\nTools:\n\nPyTorch\nTensorFlow\nScikit-Learn\nFlask\nMySQL\n\n\n\nOpen Source Contributions\n\nMesolitica-Malaysian Dataset - Scraped websites to collect over 1 MB of text data for Malaysian language Large Language Model.\n\n\n\nAwards\n\nGold Honour - International Astronomy Astrophysics Competition 2021\n\n\n\nProfessional Certificates\n\nIBM Data Science Professional Certificate\nIBM AI Engineering Professional Certificate\n\n\n\nActivities\n\nKaggle-Binary Prediction of Smoker Status using Bio-Signals (Top 28%)\nKaggle-Binary Classification with a Software Defects Dataset (Top 36%)\nKaggle-Predict Health Outcomes of Horses (Top 76%)\n\n\n\nInterests\nIn addition to my work as an AI Developer, I have a passion for astrophotography. I own a computerized telescope that is specifically designed for astrophotography. I capture images of deep sky objects, planets, and galaxies. I share these images on my Facebook page. I also enjoy participating in astronomy outreach programs. Recently, I had the opportunity to serve as a telescope operator for the Kuala Kubu Bharu Starfest 2023, which was organized by Majlis Daerah Hulu Selangor in collaboration with Sahabat Langit Utara."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "My Blog",
    "section": "",
    "text": "Greetings and a warm welcome to my blog! I’m absolutely thrilled to have you here. Within these digital pages, you’ll discover a diverse collection of articles covering a wide array of captivating topics. Feel free to explore and immerse yourself in the content. Thank you for joining me on this exciting journey!"
  },
  {
    "objectID": "blog/index.html#section",
    "href": "blog/index.html#section",
    "title": "My Blog",
    "section": "2023",
    "text": "2023\n\n\n    \n                  \n             July 7, 2023\n        \n        \n            How I Accidentally Leaked Data in My First Machine Learning Project\n            \n                \n                    data science\n                \n            \n            Dive into my early days in machine learning as I reveal a rookie mistake that led to data leakage. Learn firsthand how data leakage happens.\n        \n        \n            \n        \n    \n\n\nNo matching items\n\n\n\nThe blog post listing is based on the website source of Marvin Schmitt, who has put together an incredible listing template under CC-BY-SA 4.0 license. Thank you!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, I'm Hazman.",
    "section": "",
    "text": "Hi, I'm Hazman.\n            I am a Physics graduate from Universiti Malaya with a strong passion for data science,             AI, and machine learning. My enthusiasm for learning and applying new knowledge and skills             in the field of data science drives me to constantly seek opportunities for growth and                practical experimentation. \n            \n            Outside of my professional pursuits, I am an avid astrophotographer, and I enjoy                     exploring the cosmos through my telescope during my leisure time.   \n            \n            Let's embark on this professional adventure together!\n            \n\n        \n        \n            \n        \n        \n        \n    \n    \n        \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurrent Update\n\n\nAugust 3, 2023: I discovered the Forage, which offers job simulations from over 200 of the world’s leading companies. I applied for the KPMG AU Virtual Internship Program, which provides a virtual job simulation as a Data Analyst at KPMG AU. As a Data Analyst, I analyzed transaction datasets and recommended that companies target specific customer segments. I learned a lot from the Forage Virtual Internship Program. It was a valuable experience that allowed me to gain practical skills and knowledge in the field of data analysis.\n\n\nJuly 7, 2023: I have recently joined Kaggle, the world’s largest data science community. This platform provides me with the opportunity to learn from and collaborate with other data scientists, as well as participate in exciting machine learning competitions. I am eager to explore all that Kaggle has to offer and continue to grow in my data science journey. Join me on Kaggle and let’s go explore our data science journey together.\n\n\nJune 10, 2023: I have recently enrolled in the IBM Data Science Professional Certificate program on Coursera. This program will provide me with the opportunity to enhance my skills and knowledge in the field of data science, and I am looking forward to the learning journey ahead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  }
]