[
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Click to make it larger.\n\n\n\n|2023-12-30|Notebook|\nThe goal of this project is to detect defects in concrete structures with deep learning. The model used here is an encoder-decoder model, U-Net, developed based on EfficientNet-B1 as the backbone and trained for 60 epochs.\n\n\n\n      \n\n\n\n\n\n\n\n\n\n\n\nClick to make it larger.\n\n\n\n|2023-12-05|Competition Link|Notebook Submission|\nMy participation in a Kaggle competition Playground Series - Season 3, Episode 25, Mohs Hardness Prediction, which involves a regression problem.This competition provided me with an opportunity to learn from other experts and refine my skills. My strategy include the utilization of various tree-based models such as Gradient Boost, LightGBM, XGBoost, HistGradientBoost, and CatBoost. To enhance predictive performance, I developed a custom ensemble technique that leveraged a linear model (LADRegression) for assigning weights to the predictions generated by different models. Additionally, I incorporated an Artificial Neural Network (ANN) as a meta-learner. This component was designed to adeptly combine predictions from diverse machine learning models, contributing to the development of a robust and reliable predictive model. To validate the model’s generalization capabilities, I conducted cross-validation, ensuring its effectiveness across different datasets. Furthermore, I optimized model hyperparameters efficiently using Optuna, a tool tailored for hyperparameter tuning.\n\n\n\n      \n\n\n\n\n\n\n\n\n\n\nClick to make it larger.\n\n\n|2023-11-19|Competition Link|Notebook Submission|\nThis project is a machine learning project that we conducted for the Kaggle Playground Competition, a category of competition on Kaggle that offers short machine learning challenges. The objective of this competition was to predict the smoking status of individuals based on their bio-signals, such as systolic, cholesterol level, blood sugar, and etc. We formed a team of three members, including myself as the team leader, and we achieved a rank of 28% among 1908 teams globally. The competition provided a pre-processed dataset, so we focused on developing models with high ROC-AUC scores, which measure the ability to discriminate between smokers and non-smokers. We experimented with various algorithms, such as logistic regression, random forest, and tree-based boosting methods, including gradient boost, histogram-based gradient boost, LightGBM, XGBoost, and CatBoost. To improve AUC-ROC score and reduce variance, we created a weighted-voting ensemble that combined the three best-performing algorithms that we selected after rigorous evaluation. Our ensemble model outperformed each individual model in predicting smoking status. We fine-tuned our models using Optuna, a framework for hyperparameter optimization, to find the optimal values for each parameter. This tuning process resulted in a final submission with a ROC-AUC score of 0.87178. Although this score was slightly lower than the top submission’s 0.87946, our approach demonstrated strong predictive power and emphasized the importance of algorithm selection and ensemble techniques for reducing model variance."
  },
  {
    "objectID": "projects/index.html#computer-vision-project-concrete-defects-segmentation",
    "href": "projects/index.html#computer-vision-project-concrete-defects-segmentation",
    "title": "Projects",
    "section": "",
    "text": "Click to make it larger.\n\n\n\n|2023-12-30|Notebook|\nThe goal of this project is to detect defects in concrete structures with deep learning. The model used here is an encoder-decoder model, U-Net, developed based on EfficientNet-B1 as the backbone and trained for 60 epochs."
  },
  {
    "objectID": "projects/index.html#prediction-of-mohs-hardness-with-machine-learning",
    "href": "projects/index.html#prediction-of-mohs-hardness-with-machine-learning",
    "title": "Projects",
    "section": "",
    "text": "Click to make it larger.\n\n\n\n|2023-12-05|Competition Link|Notebook Submission|\nMy participation in a Kaggle competition Playground Series - Season 3, Episode 25, Mohs Hardness Prediction, which involves a regression problem.This competition provided me with an opportunity to learn from other experts and refine my skills. My strategy include the utilization of various tree-based models such as Gradient Boost, LightGBM, XGBoost, HistGradientBoost, and CatBoost. To enhance predictive performance, I developed a custom ensemble technique that leveraged a linear model (LADRegression) for assigning weights to the predictions generated by different models. Additionally, I incorporated an Artificial Neural Network (ANN) as a meta-learner. This component was designed to adeptly combine predictions from diverse machine learning models, contributing to the development of a robust and reliable predictive model. To validate the model’s generalization capabilities, I conducted cross-validation, ensuring its effectiveness across different datasets. Furthermore, I optimized model hyperparameters efficiently using Optuna, a tool tailored for hyperparameter tuning."
  },
  {
    "objectID": "projects/index.html#binary-prediction-of-smoker-status-using-bio-signals",
    "href": "projects/index.html#binary-prediction-of-smoker-status-using-bio-signals",
    "title": "Projects",
    "section": "",
    "text": "Click to make it larger.\n\n\n|2023-11-19|Competition Link|Notebook Submission|\nThis project is a machine learning project that we conducted for the Kaggle Playground Competition, a category of competition on Kaggle that offers short machine learning challenges. The objective of this competition was to predict the smoking status of individuals based on their bio-signals, such as systolic, cholesterol level, blood sugar, and etc. We formed a team of three members, including myself as the team leader, and we achieved a rank of 28% among 1908 teams globally. The competition provided a pre-processed dataset, so we focused on developing models with high ROC-AUC scores, which measure the ability to discriminate between smokers and non-smokers. We experimented with various algorithms, such as logistic regression, random forest, and tree-based boosting methods, including gradient boost, histogram-based gradient boost, LightGBM, XGBoost, and CatBoost. To improve AUC-ROC score and reduce variance, we created a weighted-voting ensemble that combined the three best-performing algorithms that we selected after rigorous evaluation. Our ensemble model outperformed each individual model in predicting smoking status. We fine-tuned our models using Optuna, a framework for hyperparameter optimization, to find the optimal values for each parameter. This tuning process resulted in a final submission with a ROC-AUC score of 0.87178. Although this score was slightly lower than the top submission’s 0.87946, our approach demonstrated strong predictive power and emphasized the importance of algorithm selection and ensemble techniques for reducing model variance."
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum vitae",
    "section": "",
    "text": "Download Current Resume"
  },
  {
    "objectID": "blog/231118_helloworld/index.html",
    "href": "blog/231118_helloworld/index.html",
    "title": "Hello World!",
    "section": "",
    "text": "Over the past few months, my homepage served as a placeholder for my porfolio. As I have interest in learning and sharing knowledge, I have chosen to utilize this platform to document my data science journey. And now, here we are.\nI want to express my gratitude to Murthadza Aznam, also known as @Thaza_kun on Twitter, for introducing me to the amazing Quarto project. Be sure to explore his homepage as well! Special thanks to the Quarto project for enabling this journey. Setting everything up took just a matter of hours.\nAs I aspire to become a data scientist and AI engineer, my plan is to consistently publish valuable, insightful, and thought-provoking blog posts. Cheers!"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "Get to Know Me!",
    "section": "",
    "text": "About\n\n“Develop a passion for learning. If you do, you will never cease to grow.” - Anthony J. D’Angelo\n\nI have a genuine passion for learning because I believe it’s the key to personal growth. To me, knowledge is akin to a precious gem – valuable and capable of making a positive impact on the world. This mindset has turned me into a naturally curious individual, always eager to acquire and refine skills, with a particular focus on Python, data science, machine learning, and AI.\nMy background in Physics has instilled in me the importance of understanding the fundamental principles that drive algorithms. I don’t settle for simply copying and pasting code; I strive for a deeper comprehension of how things work.\n\n“The only way to grow is to challenge yourself.” - Ashley Tisdale\n\nThis quote deeply resonates with me. Building on this philosophy, I’ve come to understand that the only way to truly grow is by challenging oneself. This realization has motivated me to actively explore unfamiliar territories. I eagerly embrace new opportunities, extending beyond my comfort zone.\nIn addition to my core focus on Python, data science, machine learning, and AI, I have explored into diverse domains. For instance, I’ve ventured into natural sciences like genetics and biology. Simultaneously, I’ve explored linguistic domains, utilizing natural language processing to navigate the intricacies of language. Furthermore, I’ve not shied away from delving into business-related applications, broadening the scope of my knowledge and skills. In addition, I also participate in various data science competitions. These experiences continually push me to improve and evolve.\n\n\nExperience\nAI Developer | October 2023 - Present\nRenaissance AI\n\nLed Research and Development team, innovating and implementing AI technologies for various applications, including product embedding and the development of a speech-to-text application.\nEngineered a recommender system architecture for the backend of an e-commerce platform, optimising product recommendations and improving user experience through the integration of reinforcement learning.\nCollaborated with a cross-functional AI backend team to analyse business requirements, skillfully translating them into scalable and efficient AI solutions that significantly contributed to client satisfaction.\n\n\n\nEducation\nUniversiti Malaya | September 2019 - March 2023\nBachelor of Science in Physics\n\nPassed with Honour\nCGPA: 3.68\n\nUniversiti Malaya | September 2018 - June 2019\nPhysical Science\n\nChemistry, Additional Mathematics, Mathematics, Physics, Programming\nCGPA: 4.00\n\n\n\nSkills\nProgramming Languages:\n\nPython\nC\nSQL\n\nTools:\n\nPyTorch\nTensorFlow\nScikit-Learn\nFlask\nMySQL\n\n\n\nOpen Source Contributions\n\nMesolitica-Malaysian Dataset - Scraped websites to collect over 50+ pages of data for Malaysian language Large Language Model.\n\n\n\nAwards\n\nGold Honour - International Astronomy Astrophysics Competition 2021\n\n\n\nCourses and Certificates\n\nGeneral Assembly - Data Scientist Bootcamp [Link]\nIBM AI Engineering Professional Certificate [Link]\nIBM Data Science Professional Certificate [Link]\nKPMG AU Data Analytics Virtual Internship [Link]\n\n\n\nActivities\n\nKaggle-Binary Prediction of Smoker Status using Bio-Signals (Top 28%) [Link]\nKaggle-Binary Classification with a Software Defects Dataset (Top 36%) [Link]\nKaggle-Predict Health Outcomes of Horses (Top 76%)\n\n\n\nInterests\nIn addition to my work as an AI Developer, I have a passion for astrophotography. I own a computerized telescope that is specifically designed for astrophotography. I capture images of deep sky objects, planets, and galaxies. I share these images on my Facebook page. I also enjoy participating in astronomy outreach programs. Recently, I had the opportunity to serve as a telescope operator for the Kuala Kubu Bharu Starfest 2023, which was organized by Majlis Daerah Hulu Selangor in collaboration with Sahabat Langit Utara."
  },
  {
    "objectID": "blog/231118_dataleakage/index.html",
    "href": "blog/231118_dataleakage/index.html",
    "title": "How I Accidentally Leaked Data in My First Machine Learning Project",
    "section": "",
    "text": "In the early stages of my journey into machine learning, I launched my first project, which was centered around classifying breast cancer. The project initially seemed successful, as I was able to achieve high accuracy during the validation phase. However, I noticed a significant drop when testing with a separate unseen dataset. It was for two months until I realized I had unintentionally leaked training data into the validation set due to improper scaling and transformation practices.\nIn this article, I will be demonstrating the common novice mistake that leads to data leakage. The mistakes I show here are exactly what I did before. To start, data leakage generally occurs when our training data is fed with information about the target, but similar data is available when the model is used in predictions. This leads to high score on the training set, but the model will perform poorly when tested with unseen data. In simple words, data leakage makes a machine learning model look very accurate until we start making predictions with new set of data to the model, and then the model becomes very inaccurate.\n\nTrain-Test Contamination\nThere are various types of data leakage, and the type I am addressing here is train-test contamination. This kind of leakage happens when the user fails to carefully distinguish between training data and validation data. For instance, during preprocessing tasks such as imputing missing values or data scaling before using train_test_split(). While the model constructed may yield a high validation score, instilling confidence, it ultimately performs poorly when tested with unseen data.\nThe dataset that I am using here is obtained from Kaggle competition dataset. In this demonstration, we will need to predict whether a software defects or not based on the features given.\n# Import required libraries and packages\nimport numpy as np                          # For linear algebra\nimport pandas as pd                         # For data manipulation\nimport matplotlib as mlt                    # For visualization\nimport matplotlib.pyplot as plt             # For visualization(scripting layer)\nimport seaborn as sns                       # For visualization\n\n# Import the data\ndf = pd.read_csv(r'/kaggle/input/playground-series-s3e23/train.csv', index_col = 'id')\n\n# Show the header of the data\ndf.head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nloc\nv(g)\nev(g)\niv(g)\nn\nv\nl\nd\ni\ne\nb\nt\nlOCode\nlOComment\nlOBlank\nlocCodeAndComment\nuniq_Op\nuniq_Opnd\ntotal_Op\ntotal_Opnd\nbranchCount\ndefects\n\n\n\n\n25.0\n6.0\n5.0\n6.0\n88.0\n461.82\n0.06\n16.92\n26.42\n7621.43\n0.15\n423.41\n19\n0\n4\n0\n18.0\n18.0\n54.0\n36.0\n11.0\nFalse\n\n\n36.0\n2.0\n1.0\n2.0\n133.0\n676.63\n0.03\n30.23\n22.23\n19091.41\n0.23\n1060.96\n27\n3\n2\n1\n16.0\n13.0\n74.0\n49.0\n3.0\nFalse\n\n\n7.0\n1.0\n1.0\n1.0\n16.0\n62.51\n0.40\n2.50\n21.59\n220.18\n0.02\n12.23\n4\n0\n1\n0\n5.0\n6.0\n11.0\n6.0\n1.0\nFalse\n\n\n22.0\n2.0\n1.0\n1.0\n94.0\n456.65\n0.09\n11.74\n39.72\n5421.87\n0.15\n301.22\n14\n0\n3\n0\n14.0\n23.0\n56.0\n36.0\n3.0\nFalse\n\n\n38.0\n5.0\n1.0\n4.0\n130.0\n644.05\n0.04\n25.91\n23.55\n15572.12\n0.21\n865.12\n22\n7\n4\n0\n15.0\n17.0\n74.0\n51.0\n9.0\nTrue\n\n\n\nFor demonstration purposes, I skipped much of the data preparation work. Now, in this dataset, I aim to log-transform the data since most features are right-skewed. However, here’s where the mistake occurred: in my attempt to perform the log-transform, I applied the transformation to the entire dataset, df.\n# Choose all the numerical columns\nnum_cols = [col for col in df.columns if col not in [\"defects\"]]\ndf_num = df[num_cols]\n\n# Apply log transformation to the numerical columns\ndf_num_transformed = np.log1p(df_num)\n\n# Concatenate the transformed numerical columns with the \"defects\" column\ndf_transformed = pd.concat([df_num_transformed, df[\"defects\"]], axis=1)\n\ndf_transformed.head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nloc\nv(g)\nev(g)\niv(g)\nn\nv\nl\nd\ni\ne\nb\nt\nlOCode\nlOComment\nlOBlank\nlocCodeAndComment\nuniq_Op\nuniq_Opnd\ntotal_Op\ntotal_Opnd\nbranchCount\ndefects\n\n\n\n\n3.258097\n1.94591\n1.79176\n1.94591\n4.48864\n6.13734\n0.05827\n2.88592\n3.31127\n8.93885\n0.13976\n6.05070\n2.99573\n0.00000\n1.60944\n0.00000\n2.94444\n2.94444\n4.00733\n3.61092\n2.48491\nFalse\n\n\n3.61092\n1.09861\n0.69315\n1.09861\n4.89784\n6.51860\n0.02956\n3.44138\n3.14545\n9.85705\n0.20701\n6.96787\n3.33221\n1.38629\n1.09861\n0.69315\n2.83321\n2.63906\n4.31749\n3.91202\n1.38629\nFalse\n\n\n2.07944\n0.69315\n0.69315\n0.69315\n2.83321\n4.15120\n0.33647\n1.25276\n3.11751\n5.39898\n0.01980\n2.58249\n1.60944\n0.00000\n0.69315\n0.00000\n1.79176\n1.94591\n2.48491\n1.94591\n0.69315\nFalse\n\n\n3.13549\n1.09861\n0.69315\n0.69315\n4.55388\n6.12610\n0.08618\n2.54475\n3.70672\n8.59838\n0.13976\n5.71116\n2.70805\n0.00000\n1.38629\n0.00000\n2.70805\n3.17805\n4.04305\n3.61092\n1.38629\nFalse\n\n\n3.66356\n1.79176\n0.69315\n1.60944\n4.87520\n6.46933\n0.03922\n3.29250\n3.20071\n9.65330\n0.19062\n6.76402\n3.13549\n2.07944\n1.60944\n0.00000\n2.77259\n2.89037\n4.31749\n3.95124\n2.30259\nTrue\n\n\n\nAs we can observe, the entire dataset has now been log-transformed. Now, let’s proceed to develop our classification model using logistic regression.\n# Import libraries and packages\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n# Make empty dictionaries to store accuracy\nval_accuracy_dict = {}\ntest_accuracy_dict = {}\n\n# Separate the data into predictor (X) and target (Y)\nX = df_transformed.drop('defects',axis=1)\nY = df_transformed['defects'].values\n \n# Split the dataset into training and validation set\nx_train1, x_val1, y_train1, y_val1 = train_test_split(X, Y, stratify=Y, test_size=0.10, random_state=45)\n\n# Initialize and train the logistic regression model. I include 1 to indicate it is Case 1/Model 1\nmodel1 = LogisticRegression()\nmodel1.fit(x_train1, y_train1)\n\n# Make predictions on the validation set\ny_pred1 = model1.predict(x_val1)\n\n# Calculate accuracy\naccuracy1 = accuracy_score(y_val1, y_pred1)\n\n# Print and store the accuracy in a dictionary\nprint(\"Accuracy:\", accuracy1)\nval_accuracy_dict['model 1'] = accuracy1\nAccuracy: 0.8136592556197028\nIn Case 1, where we transformed the entire dataset, we achieved a validation accuracy of 81%. In practice, it is advisable to perform cross-validation to ensure the reliability of our accuracy score, although we will skip this step for now. We can anticipate obtaining a similar accuracy of around 81% when using this model to predict unseen data.\nNow, let’s proceed to Case 2, where we followed good practices in transforming the dataset.\n# Separate the data into predictor (X) and target (Y)\nX = df.drop('defects',axis=1)\nY = df['defects']\n\n# Split the dataset into training and validation set. I include 2 to indicate it is Case 2/Model 2\nx_train2, x_val2, y_train2, y_val2 = train_test_split(X, Y, stratify=Y, test_size=0.10, random_state=45)\n\n# Choose all the numerical columns\nnum_cols2 = [col for col in x_train2.columns]\nnumerical_columns2 = x_train2[num_cols2] # Notice here I only select the training set instead of the whole dataset df\n\n# Apply log transformation to the numerical columns\nx_train2 = np.log1p(numerical_columns2)\n\n# Initialize and train the logistic regression model\nmodel2 = LogisticRegression()\nmodel2.fit(x_train2, y_train2)\n\n# Make predictions on the validation set\ny_pred2 = model2.predict(x_val2)\n\n# Calculate accuracy\naccuracy2 = accuracy_score(y_val2, y_pred2)\n\n# Print and store the accuracy in a dictionary\nprint(\"Accuracy:\", accuracy2)\nval_accuracy_dict['model 2'] = accuracy2\nAccuracy: 0.7716496744871637\nOh, what a surprise! Our “good practice” model performed worse on the validation set. Shocking, right? Here, we can observe that the validation accuracy of Model 2 is lower compared to the validation accuracy of Model 1. Let’s now evaluate the performance of both Model 1 and Model 2 with unseen data.\nCase 1\n# Use Model 1 to predict unseen data\npred1 = model1.predict(x_test)\n\n# Print the test accuracy score\nprint(\"Accuracy:\", accuracy_score(y_test, pred1))\ntest_accuracy_dict['model 1'] = accuracy_score(y_test, pred1)\nAccuracy: 0.768387952635975\nCase 2\n# Use Model 2 to predict unseen data\npred2 = model2.predict(x_test)\n\n# Print the test accuracy score\nprint(\"Accuracy:\", accuracy_score(y_test, pred2))\ntest_accuracy_dict['model 2'] = accuracy_score(y_test, pred2)\nAccuracy: 0.768387952635975\n\n\n\nValidation vs Test Accuracy for Model 1 and Model 2.\n\n\nThe test accuracy for both of our models appears to be the same. However, it’s crucial to note the difference in accuracy for Model 1 between the validation set and the test set. There is no difference in Model 2, which adhered to the good practice of transforming the training set only.\nThis discrepancy highlights the impact of data leakage. When data leakage occurs, the accuracy obtained during the model’s development phase tends to be overly optimistic, resulting in a high score during evaluation. However, when the model is used to predict unseen data, its performance is notably worse, similar to the scenario in Case 1.\nDuring model development, the score obtained should ideally reflect what can be expected when predicting unseen data. Experiencing a drop in accuracy during deployment, as seen in Case 1, is problematic, particularly when the model is intended for business purposes. This underscores the importance of correct data handling and following good practices, as demonstrated by Model 2, to ensure the model’s reliability in real-world applications.\nWe can enhance the practice of Model 2 by implementing a Pipeline, a topic I will explore into later. For now, I aim to illustrate the occurrence of data leakage, a rookie mistake I made during my early days in machine learning."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "DataWithHazman",
    "section": "",
    "text": "Welcome to my blog DataWithHazman. I am excited that you are here! You will find a collection of articles on a wide variety of topics. As a learner, I want to share with you what I pick up along the way. I hope you will find something of interest in my writing. So, take a look around and let me know what you think."
  },
  {
    "objectID": "blog/index.html#section",
    "href": "blog/index.html#section",
    "title": "DataWithHazman",
    "section": "2023",
    "text": "2023\n\n\n    \n                  \n             November 19, 2023\n        \n        \n            How I Accidentally Leaked Data in My First Machine Learning Project\n            \n                \n                    data science\n                \n            \n            Exploring into my early days in machine learning as I am sharing a rookie mistake that led to data leakage. Learn firsthand how data leakage happens.\n        \n        \n            \n        \n    \n    \n                  \n             November 18, 2023\n        \n        \n            Hello World!\n            \n                \n                    news\n                \n            \n            Over the past few months, my homepage served as a placeholder for my porfolio. As I have interest in learning and sharing knowledge, I have chosen to utilize this platform to document my data science journey. And now, here we are.\n        \n        \n            \n        \n    \n\n\nNo matching items\n\n\n\nThe blog post listing is based on the website source of Marvin Schmitt, who has put together an incredible listing template under CC-BY-SA 4.0 license. Thank you!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, I'm Hazman.",
    "section": "",
    "text": "I am a Physics graduate from Universiti Malaya with a strong passion for data science,             AI, and machine  learning. My enthusiasm for learning and applying new knowledge and skills             in the field of data science drives me to constantly seek opportunities for growth and                practical experimentation. \n            \n            Outside of my professional pursuits, I am an avid astrophotographer, and I enjoy                     exploring the cosmos through my telescope during my leisure time.   \n            \n            Let's embark on this professional adventure together!"
  },
  {
    "objectID": "index.html#current-updates",
    "href": "index.html#current-updates",
    "title": "Hi, I'm Hazman.",
    "section": "Current Updates",
    "text": "Current Updates\n\nOctober 23, 2023: Great news! I have begun my on-the-job training as an AI Developer, which is my first baby-step for my data science career. I am part of the AI Backend team that works on developing AI-based applications for E-commerce. I also get to work with other areas of AI such as NLP and LLM. This training will go on for three months.\nSeptember 9, 2023: I have been selected for the K-Youth Development Programme, a data science upskilling program offered by Excelerate Asia in partnership with General Assembly. This program is sponsored by Khazanah Nasional and lasts for four months. The first month is dedicated to learning data science skills through a bootcamp, while the remaining three months involve on-the-job training. The program also helps me develop career skills such as resume writing, interviewing, and communication.\nJuly 7, 2023: I have recently joined Kaggle, the world’s largest data science community. This platform provides me with the opportunity to learn from and collaborate with other data scientists, as well as participate in exciting machine learning competitions. I am eager to explore all that Kaggle has to offer and continue to grow in my data science journey. Join me on Kaggle and let’s go explore our data science journey together.\nJune 10, 2023: I made a career choice to pursue data science. Since I had a physics background and not a computer science one, I began to look for resources that would help me start my journey in data science. I enrolled in the IBM Data Science Professional Certificate program on Coursera which would enable me to improve my skills and knowledge in this field."
  },
  {
    "objectID": "index.html#featured-blog-posts",
    "href": "index.html#featured-blog-posts",
    "title": "Hi, I'm Hazman.",
    "section": "Featured Blog Posts",
    "text": "Featured Blog Posts\n\n\n\n\n\n\n\n\n\n\nHow I Accidentally Leaked Data in My First Machine Learning Project\n\n\n\nNov 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHello World!\n\n\n\nNov 18, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  }
]